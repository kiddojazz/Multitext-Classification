{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84233db0-c203-4c97-b13f-da554d1bf9dc",
   "metadata": {},
   "source": [
    "# Step 1: Import All Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc1acc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665d1c13-fc74-4b8a-857e-ba0ed4d1e3a2",
   "metadata": {},
   "source": [
    "## Step 2: Read File from GitHub Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ade29ad4-a8d6-4389-9cd1-6988afed84b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                data         labels\n",
      "0  Musicians to tackle US red tape  Musicians gro...  entertainment\n",
      "1  U2s desire to be number one  U2, who have won ...  entertainment\n",
      "2  Rocker Doherty in on-stage fight  Rock singer ...  entertainment\n",
      "3  Snicket tops US box office chart  The film ada...  entertainment\n",
      "4  Oceans Twelve raids box office  Oceans Twelve,...  entertainment\n"
     ]
    }
   ],
   "source": [
    "url = 'https://github.com/kiddojazz/Multitext-Classification/blob/master/bbc_data.csv?raw=true'\n",
    "#df = pd.read_csv(url,index_col=0)\n",
    "df = pd.read_csv(url)\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e298b21-b972-43c2-8e32-bbcd8cc0c890",
   "metadata": {},
   "source": [
    "## Step 3: Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c88be4e7-5a08-4276-ab1d-54a32929303c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['entertainment', 'unknown', 'business', 'sport', 'politics',\n",
       "       'tech'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the Unique Items from the label column\n",
    "df['labels'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077ca057-1bc8-42aa-bef0-dc55ad5057be",
   "metadata": {},
   "source": [
    "### Filter out Unknown category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29b65a2f-1309-42c4-baa3-c28f6caeed22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                data         labels\n",
      "0  Musicians to tackle US red tape  Musicians gro...  entertainment\n",
      "1  U2s desire to be number one  U2, who have won ...  entertainment\n",
      "2  Rocker Doherty in on-stage fight  Rock singer ...  entertainment\n",
      "3  Snicket tops US box office chart  The film ada...  entertainment\n",
      "4  Oceans Twelve raids box office  Oceans Twelve,...  entertainment\n"
     ]
    }
   ],
   "source": [
    "# Filter out rows where the labels are 'unknown'\n",
    "df = df[df['labels'] != 'unknown']\n",
    "\n",
    "# Print the filtered DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5812ba31-5da4-4dd0-bbe3-2c625701c0db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['entertainment', 'business', 'sport', 'politics', 'tech'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the Unique Items from the label column\n",
    "df['labels'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58229cfa-3e55-4782-b72d-dcbcbb87af07",
   "metadata": {},
   "source": [
    "### Encode Label Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "304c51a0-885f-453d-9b7b-2cad6d94bb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                data         labels  \\\n",
      "0  Musicians to tackle US red tape  Musicians gro...  entertainment   \n",
      "1  U2s desire to be number one  U2, who have won ...  entertainment   \n",
      "2  Rocker Doherty in on-stage fight  Rock singer ...  entertainment   \n",
      "3  Snicket tops US box office chart  The film ada...  entertainment   \n",
      "4  Oceans Twelve raids box office  Oceans Twelve,...  entertainment   \n",
      "\n",
      "   encoded_cat  \n",
      "0            1  \n",
      "1            1  \n",
      "2            1  \n",
      "3            1  \n",
      "4            1  \n"
     ]
    }
   ],
   "source": [
    "#Encode label for easy identification.\n",
    "df['encoded_cat'] = df['labels'].astype('category').cat.codes\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04a9405d-235f-4456-8035-bcdcc729f9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 3, 2, 4], dtype=int8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['encoded_cat'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc3046c7-67fd-4775-a1bc-36b5ffb969a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   data labels  encoded_cat\n",
      "1824  Mobiles rack up 20 years of use  Mobile phones...   tech            4\n",
      "1825  Broadband steams ahead in the US  More and mor...   tech            4\n",
      "1826  EA to take on film and TV giants  Video game g...   tech            4\n",
      "1827  Microsoft releases patches  Microsoft has warn...   tech            4\n",
      "1828  China to overtake US net use  The Chinese net-...   tech            4\n",
      "...                                                 ...    ...          ...\n",
      "2220  Warning over Windows Word files  Writing a Mic...   tech            4\n",
      "2221  Fast lifts rise into record books  Two high-sp...   tech            4\n",
      "2222  Nintendo adds media playing to DS  Nintendo is...   tech            4\n",
      "2223  Fast moving phone viruses appear  Security fir...   tech            4\n",
      "2224  Hacker threat to Apples iTunes  Users of Apple...   tech            4\n",
      "\n",
      "[364 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "filtered_df = df[df['encoded_cat'] == 4]\n",
    "\n",
    "# Print the filtered DataFrame\n",
    "print(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61787fe5-0ebc-4f4b-89f4-b6bb4d359ee3",
   "metadata": {},
   "source": [
    "Business = 0, Entertainment = 1, Politics = 2, Sport = 3, Tech = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26942dfa-c360-44af-808d-df1761fae7a5",
   "metadata": {},
   "source": [
    "## Step 4: Model Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe41c51d-34d2-415a-90ae-c21159b6176f",
   "metadata": {},
   "source": [
    "### Split DataFrame to Feature and Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18dd283b-6be0-418d-9aa2-e3f66313e07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_texts = df[\"data\"].to_list() # Features (not-tokenized yet)\n",
    "data_labels = df[\"encoded_cat\"].to_list() # Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54806d74-bd6d-4823-ade0-aba10c2ef92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install scikit-learn\n",
    "#!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39849fdb-f2d7-419b-90a1-4abcaf4ff224",
   "metadata": {},
   "source": [
    "### Split to Train and Test using SKlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db913a76-baa6-41fe-932d-231580d81bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split Train and Validation data\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(data_texts, data_labels, test_size=0.2, random_state=0, shuffle=True)\n",
    "\n",
    "# Keep some data for inference (testing)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(train_texts, train_labels, test_size=0.01, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08661a97-c840-453b-9a53-7bbed9a3998d",
   "metadata": {},
   "source": [
    "### Download Model needed from Transformers Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f25b549e-6ce0-41dd-9f05-2c38d3a0ce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
    "# val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b502ae-583a-47b1-8e8e-1196690b9fd5",
   "metadata": {},
   "source": [
    "### Create TensorFlow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dffe412e-e6ba-495e-b51f-acf2b6998871",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    "))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_labels\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a5a75b-b570-4471-bcc5-183d04623830",
   "metadata": {},
   "source": [
    "### TFTrainer class for Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c270f1a0-0be7-4d8d-9590-5c4a86347f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Temidayo\\anaconda3\\envs\\tfp3.11\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=5)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5, epsilon=1e-08)\n",
    "model.compile(optimizer=optimizer, loss=model.hf_compute_loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0e386d-0a88-4f1e-aaff-61de99e2e405",
   "metadata": {},
   "source": [
    "### Train model using Tensorflow:\n",
    "This was set to 2 epochs or iteration, for better accuracy we might need to increase the number of iteration and parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ddf3fff-f608-4c78-96ab-2a195ae9a470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x000001D6715089A0> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function infer_framework at 0x000001D6715089A0> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:From C:\\Users\\Temidayo\\anaconda3\\envs\\tfp3.11\\Lib\\site-packages\\tensorflow\\python\\autograph\\converters\\directives.py:126: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Temidayo\\anaconda3\\envs\\tfp3.11\\Lib\\site-packages\\tensorflow\\python\\autograph\\converters\\directives.py:126: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Temidayo\\anaconda3\\envs\\tfp3.11\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "101/101 [==============================] - 2859s 28s/step - loss: 0.4363 - accuracy: 0.9072 - val_loss: 0.0782 - val_accuracy: 0.9878\n",
      "Epoch 2/2\n",
      "101/101 [==============================] - 1460s 14s/step - loss: 0.0792 - accuracy: 0.9821 - val_loss: 0.0759 - val_accuracy: 0.9780\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x1d60d960a90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "model.fit(train_dataset.shuffle(1000).batch(16),\n",
    "          epochs=2,\n",
    "          batch_size=16,\n",
    "          validation_data=val_dataset.shuffle(1000).batch(16),\n",
    "         callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "caceec1d-8e0f-4c77-9c3a-e8ea06f3c416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMa  multiple                  66362880  \n",
      " inLayer)                                                        \n",
      "                                                                 \n",
      " pre_classifier (Dense)      multiple                  590592    \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  3845      \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66957317 (255.42 MB)\n",
      "Trainable params: 66957317 (255.42 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Display the model's architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a6e2bd-baca-4500-a94b-0aba98e2790c",
   "metadata": {},
   "source": [
    "### Save Trained Model in Local Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0115c411-41ad-4fbc-a83c-64c1bb3c1cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73c88bf1-470a-4491-8ed2-b4e5573ec5fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Multitext_Classification_new\\\\tokenizer_config.json',\n",
       " 'Multitext_Classification_new\\\\special_tokens_map.json',\n",
       " 'Multitext_Classification_new\\\\vocab.txt',\n",
       " 'Multitext_Classification_new\\\\added_tokens.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_directory = \"Multitext_Classification_new\" # Change this to your preferred location\n",
    "\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2237cda2-8d51-47ac-8968-5b0cbdfe36ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at Multitext_Classification_new were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at Multitext_Classification_new and are newly initialized: ['dropout_39']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "save_directory = \"Multitext_Classification_new\"\n",
    "loaded_tokenizer = DistilBertTokenizer.from_pretrained(save_directory)\n",
    "loaded_model = TFDistilBertForSequenceClassification.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cfcf7b-33ca-41b2-a781-261b0f6b0685",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdfa80b4-76b3-4da6-af3d-497df13ca969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mobiles double up as bus tickets  Mobiles could soon double up as travel cards, with Nokia planning to try out a wireless ticket system on German buses.  Early next year travellers in the city of Hanau, near Frankfurt, will be able to pay for tickets by passing their phone over a smart-card reader already installed on the buses. Passengers will need to own a Nokia 3220 handset which will have a special shell attached to it. The system would reduce queues and make travelling easier, said Nokia.  Transport systems around the world are seeing the advantage of using ticketless smartcards. Using a mobile phone is the next step, said Gerhard Romen, head of market development at Nokia.  The ticketless trial will start early in 2005 and people will also be able to access transport information and timetables via their phones. Nokia has worked with electronics giant Philips to develop a shell for the mobile phone that will be compatible with Hanaus existing ticketing system. The system opens up possibilities for mobile devices to be interact with everyday environments, said Mr Romen. \"It could be used in shops to get product information, at bus-stops to get information about the next bus or, for example, by being passed over an advert of a rock star to find out details of concerts or get ringtones,\" he told the BBC News website. He is confident that the trial being run in Germany could be extended to transport systems in other countries. \"The technology offers access to a lot of services and makes it easy to get the information you want,\" he said. '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text = test_texts[10]\n",
    "test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22cf6120-64af-4d44-8e86-b997aac3f3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_text = ['Oscar nominee Dan OHerlihy dies  Irish actor Dan OHerlihy, who was nominated for best actor at the 1955 Oscars, has died at the age of 85.  OHerlihy, whose Oscar nomination was for Luis Bunuels The Adventures of Robinson Crusoe, died at his home in Malibu, California, a spokesman said. The actor began his film career in the 1940s, playing Macduff to Orson Welles Macbeth in 1948, and was also a regular in on the Dublin stage. He later appeared in Robocop and its sequel and cult TV show Twin Peaks. He played the CEO of Omni Consumer Products in 1987s Robocop and Robocop 2 three years later, and was saw mill owner Andrew Packard in Twin Peaks, also in 1990. Despite his Oscar nomination, he had few other lead roles and became a familiar supporting actor on TV and in film. The year he was nominated, the Academy Award was won by Marlon Brando for On the Waterfront. \"']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af95bf34-e872-49b8-8bb9-4cab2e9889ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_input = loaded_tokenizer.encode(test_text,\n",
    "                                 truncation=True,\n",
    "                                 padding=True,\n",
    "                                 return_tensors=\"tf\")\n",
    "\n",
    "output = loaded_model(predict_input)[0]\n",
    "\n",
    "prediction_value = tf.argmax(output, axis=1).numpy()[0]\n",
    "prediction_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88947a4-1b2a-45b0-a1d1-33da510440da",
   "metadata": {},
   "source": [
    "### Categories Output\n",
    "Business = 0, Entertainment = 1, Politics = 2, Sport = 3, Tech = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "178b5a0f-0c65-4b61-af66-adeda8a351cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖Predicted Category: Tech\n"
     ]
    }
   ],
   "source": [
    "predict_input = loaded_tokenizer.encode(test_text,\n",
    "                                 truncation=True,\n",
    "                                 padding=True,\n",
    "                                 return_tensors=\"tf\")\n",
    "\n",
    "output = loaded_model(predict_input)[0]\n",
    "\n",
    "prediction_value = tf.argmax(output, axis=1).numpy()[0]\n",
    "\n",
    "# Convert numeric prediction to category label\n",
    "if prediction_value == 0:\n",
    "    prediction_label = \"Business\"\n",
    "elif prediction_value == 1:\n",
    "    prediction_label = \"Entertainment\"\n",
    "elif prediction_value == 2:\n",
    "    prediction_label = \"Politics\"\n",
    "elif prediction_value == 3:\n",
    "    prediction_label = \"Sport\"\n",
    "else:\n",
    "    prediction_label = \"Tech\"  # Handle unexpected values if necessary\n",
    "\n",
    "print(\"🤖Predicted Category:\", prediction_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7539f8-2a06-4445-a857-6a1cce8d3f8a",
   "metadata": {},
   "source": [
    "**Business = 0, Entertainment = 1, Politics = 2, Sport = 3, Tech = 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "859eee16-d3b5-468b-a17d-d70c2568192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_category(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2524fd13-5e98-4a4d-ab0c-06d98ff9f0d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36fbfb39-bc53-42b0-b1ff-847687a2f9d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer 'embeddings' (type TFEmbeddings).\n\n{{function_node __wrapped__ResourceGather_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[0,512] = 512 is not in [0, 512) [Op:ResourceGather] name: \n\nCall arguments received by layer 'embeddings' (type TFEmbeddings):\n  • input_ids=tf.Tensor(shape=(1, 682), dtype=int32)\n  • position_ids=None\n  • inputs_embeds=None\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m texts \u001b[38;5;129;01min\u001b[39;00m test_texts:\n\u001b[1;32m---> 16\u001b[0m     y_pred\u001b[38;5;241m.\u001b[39mappend(predict_category(texts))\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# -------------------------------------------\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix\n",
      "Cell \u001b[1;32mIn[22], line 8\u001b[0m, in \u001b[0;36mpredict_category\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_category\u001b[39m(text):\n\u001b[0;32m      3\u001b[0m     predict_input \u001b[38;5;241m=\u001b[39m loaded_tokenizer\u001b[38;5;241m.\u001b[39mencode(text,\n\u001b[0;32m      4\u001b[0m                                  truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      5\u001b[0m                                  padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      6\u001b[0m                                  return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m     output \u001b[38;5;241m=\u001b[39m loaded_model(predict_input)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     10\u001b[0m     prediction_value \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39margmax(output, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prediction_value\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tfp3.11\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tfp3.11\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:437\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    434\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    436\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[1;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munpacked_inputs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tfp3.11\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py:813\u001b[0m, in \u001b[0;36mTFDistilBertForSequenceClassification.call\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, labels, training)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;129m@unpack_inputs\u001b[39m\n\u001b[0;32m    789\u001b[0m \u001b[38;5;129m@add_start_docstrings_to_model_forward\u001b[39m(DISTILBERT_INPUTS_DOCSTRING\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size, sequence_length\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    790\u001b[0m \u001b[38;5;129m@add_code_sample_docstrings\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    805\u001b[0m     training: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    806\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[TFSequenceClassifierOutput, Tuple[tf\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[0;32m    807\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;124;03m    labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;124;03m        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;124;03m        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;124;03m        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 813\u001b[0m     distilbert_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistilbert(\n\u001b[0;32m    814\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    815\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    816\u001b[0m         head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    817\u001b[0m         inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    818\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    819\u001b[0m         output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    820\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    821\u001b[0m         training\u001b[38;5;241m=\u001b[39mtraining,\n\u001b[0;32m    822\u001b[0m     )\n\u001b[0;32m    823\u001b[0m     hidden_state \u001b[38;5;241m=\u001b[39m distilbert_output[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     pooled_output \u001b[38;5;241m=\u001b[39m hidden_state[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tfp3.11\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:437\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    434\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    436\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[1;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munpacked_inputs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tfp3.11\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py:458\u001b[0m, in \u001b[0;36mTFDistilBertMainLayer.call\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    456\u001b[0m     head_mask \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_hidden_layers\n\u001b[1;32m--> 458\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(input_ids, inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m    459\u001b[0m tfmr_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[0;32m    460\u001b[0m     embedding_output,\n\u001b[0;32m    461\u001b[0m     attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    466\u001b[0m     training\u001b[38;5;241m=\u001b[39mtraining,\n\u001b[0;32m    467\u001b[0m )\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tfmr_output\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tfp3.11\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_tf_distilbert.py:121\u001b[0m, in \u001b[0;36mTFEmbeddings.call\u001b[1;34m(self, input_ids, position_ids, inputs_embeds, training)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m position_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(tf\u001b[38;5;241m.\u001b[39mrange(start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, limit\u001b[38;5;241m=\u001b[39minput_shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m position_embeds \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mgather(params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings, indices\u001b[38;5;241m=\u001b[39mposition_ids)\n\u001b[0;32m    122\u001b[0m final_embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m position_embeds\n\u001b[0;32m    123\u001b[0m final_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(inputs\u001b[38;5;241m=\u001b[39mfinal_embeddings)\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer 'embeddings' (type TFEmbeddings).\n\n{{function_node __wrapped__ResourceGather_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[0,512] = 512 is not in [0, 512) [Op:ResourceGather] name: \n\nCall arguments received by layer 'embeddings' (type TFEmbeddings):\n  • input_ids=tf.Tensor(shape=(1, 682), dtype=int32)\n  • position_ids=None\n  • inputs_embeds=None\n  • training=False"
     ]
    }
   ],
   "source": [
    "def predict_category(text):\n",
    "\n",
    "    predict_input = loaded_tokenizer.encode(text,\n",
    "                                 truncation=True,\n",
    "                                 padding=True,\n",
    "                                 return_tensors=\"tf\")\n",
    "\n",
    "    output = loaded_model(predict_input)[0]\n",
    "\n",
    "    prediction_value = tf.argmax(output, axis=1).numpy()[0]\n",
    "\n",
    "    return prediction_value\n",
    "# -----------------------------------------------------\n",
    "# y_pred = []\n",
    "# for texts in test_texts:\n",
    "#     y_pred.append(predict_category(texts))\n",
    "predict_category(test_texts)\n",
    "# -------------------------------------------\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "confusion = confusion_matrix(test_labels, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set(font_scale=1.2)\n",
    "sns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, square=True,\n",
    "            xticklabels=[\"Business\", \"Entertainment\", \"Politics\", \"Sport\", \"Tech\"], yticklabels=[\"Business\", \"Entertainment\", \"Politics\", \"Sport\", \"Tech\"])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb17562d-b993-440a-8acf-1190c83a95cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cf86bd-29d3-45f1-af3d-4d6134ba47ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfc144b-a71c-460c-ac6a-d58628257205",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8cc941-45c7-4869-8e2b-adb9813faadb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557f0053-d0ac-455b-ad40-69b8f98d9374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64c4bf3-d4a4-4d06-9e66-27478ec56175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eb61ca-ccd6-4e99-ba94-f60b4e22c7c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4cbc9f-adfc-47f3-a13c-ebc2b76a73da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac309465-b8bb-4fd3-af7b-51a91124b8e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56fd31f7-3b67-42bb-a040-ac085e60fa54",
   "metadata": {},
   "source": [
    "## Test with Unknown Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2029cd8f-8369-4f10-b827-9a439f6772bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                data         labels\n",
      "0  Musicians to tackle US red tape  Musicians gro...  entertainment\n",
      "1  U2s desire to be number one  U2, who have won ...  entertainment\n",
      "2  Rocker Doherty in on-stage fight  Rock singer ...  entertainment\n",
      "3  Snicket tops US box office chart  The film ada...  entertainment\n",
      "4  Oceans Twelve raids box office  Oceans Twelve,...  entertainment\n"
     ]
    }
   ],
   "source": [
    "url_unknown = 'https://github.com/kiddojazz/Multitext-Classification/blob/master/bbc_data.csv?raw=true'\n",
    "df_unknown = pd.read_csv(url)\n",
    "print(df_unknown.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ac6843b-3600-4731-8c45-5e8e1c6489df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 data   labels\n",
      "6   Pete Doherty misses bail deadline  Singer Pete...  unknown\n",
      "33  Bookmakers back Aviator for Oscar  The Aviator...  unknown\n",
      "42  Singer Christina Aguilera to wed  Pop star Chr...  unknown\n",
      "51  Elvis set to top UK singles chart  Rock n roll...  unknown\n",
      "53  Fantasy book wins Hollywood deal  A British au...  unknown\n"
     ]
    }
   ],
   "source": [
    "df_unknown = df_unknown[df_unknown['labels'] == 'unknown']\n",
    "print(df_unknown.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "deb25147-037a-41ac-931f-e8fd45e81317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3976606e-7352-4155-9112-ca05346a72f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
